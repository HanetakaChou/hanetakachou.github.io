<!doctype html>
<html>
<head>
<meta charset="utf-8">
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/github-markdown-css/4.0.0/github-markdown.min.css">
<link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/highlightjs/cdn-release/build/styles/default.min.css">
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex/dist/katex.min.css">
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/markdown-it-texmath/css/texmath.min.css">

</head>
<body class="markdown-body">
<h1 id="streaming">Streaming</h1>
<h2 id="streaming-thread">Streaming thread</h2>
<p>The &quot;pt_texture_read_input_stream&quot; consists of two stages. The first stage is a serial stage which is executed by the caller thread due to the limit of the serial slob allocator(TODO: find a parallel alternative) while the second stage is to spawn a TBB task(Technically, this task is not spawned but enqueued to the same task arena to make sure the value returned by &quot;tbb::this_task_arena::current_thread_id&quot; is unique) which uses the &quot;tbb::this_task_arena::current_thread_id&quot; to select the proper &quot;command_pool&quot; and &quot;command_buffer&quot;.</p>
<h2 id="main-rendering-thread">Main rendering thread</h2>
<p>The &quot;pt_gfx_connection_draw_acquire&quot; calls the &quot;reduce_streaming_task&quot; which can be considered as the third stage after the second stage of the &quot;pt_texture_read_input_stream&quot;. In fact, the whole streaming process can be considered as a &quot;serial-parallel-serial&quot; parallel pipeline since the implemention of the &quot;parallel-serial&quot; part of the parallel pipeline is intrinsically equivalent to the parallel reduce.</p>
<h2 id="performance-tuning">Performance tuning</h2>
<p>There are three variables which should be tuning according to the specific project.</p>
<h3 id="transfersrcbuffersize">transfer_src_buffer_size</h3>
<p>The &quot;transfer_src_buffer_size&quot; is size of the staging buffer of this engine. When there is no enough memory, the spawned TBB task will be pushed into the &quot;streaming_task_respawn_list&quot;. If you find the count of the &quot;streaming_task_respawn&quot; is too high, you may increase the size of the &quot;transfer_src_buffer&quot;.</p>
<h3 id="streamingtaskrespawnlinearlistcount">STREAMING_TASK_RESPAWN_LINEAR_LIST_COUNT</h3>
<p>The &quot;streaming_task_respawn_list&quot; is a &quot;mp_list&quot; which consists of two parts: a &quot;linear_list&quot; and a &quot;link_list&quot;. Ideally, the &quot;link_list&quot; should rarely be used since it's harmful to the performance. If you find the count of the &quot;link_list&quot; is too high , you may increase the value of the &quot;STREAMING_TASK_RESPAWN_LINEAR_LIST_COUNT&quot;.</p>
<h3 id="streamingobjectlinearlistcount">STREAMING_OBJECT_LINEAR_LIST_COUNT</h3>
<p>The &quot;streaming_object_list&quot; contains the objects which have completed the second stage or have been cancelled or in error during the second stage. This list can considered as a &quot;input_item_buffer&quot; between the second stage and the third stage of the parallel pipeline. Evidently, the size of this list will impact the throughput. And this list is a &quot;mp_list&quot; as well. Similarly, you should try to avoid the usage of the &quot;link_list&quot; by increasing the value of the &quot;STREAMING_OBJECT_LINEAR_LIST_COUNT&quot;.</p>

</body>
</html>